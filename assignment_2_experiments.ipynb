{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Dense, \n",
    "    Dropout, Flatten, Reshape, LSTM, Bidirectional, \n",
    "    GlobalAveragePooling1D, GlobalAveragePooling2D, \n",
    "    Input, Lambda\n",
    ")\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# Check if GPU is available\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Recurrent Neural Networks (CRNN)\n",
    "A CRNN combines both convolutional layers (for feature extraction from spectrograms) and recurrent layers (to capture temporal dependencies). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CNN layers for feature extraction\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Reshape for recurrent layers\n",
    "    model.add(Reshape((-1, model.output_shape[3])))\n",
    "    \n",
    "    # RNN layers for temporal dynamics\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Based Audio Classification\n",
    "Attention mechanisms extend traditional neural networks by allowing the model to focus on the most relevant parts of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attention_audio_model(input_shape, num_classes):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN feature extraction\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Reshape for sequence processing\n",
    "    x = Reshape((-1, x.shape[3]))(x)\n",
    "    \n",
    "    # Self-attention mechanism\n",
    "    query = Dense(64)(x)\n",
    "    key = Dense(64)(x)\n",
    "    value = Dense(64)(x)\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    attention_scores = tf.matmul(query, key, transpose_b=True)\n",
    "    attention_scores = attention_scores / tf.math.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "    context_vector = tf.matmul(attention_weights, value)\n",
    "    \n",
    "    # Global pooling and classification\n",
    "    x = GlobalAveragePooling1D()(context_vector)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with Audio Transformers\n",
    "Pre-trained audio transformers like Wav2Vec2 represent a significant extension beyond traditional models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "import torch.nn as nn\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Load pre-trained Wav2Vec2 model\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        # Freeze the feature extractor\n",
    "        for param in self.wav2vec.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Add classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = self.wav2vec(x).last_hidden_state\n",
    "        # Global pooling\n",
    "        pooled = features.mean(dim=1)\n",
    "        # Classification\n",
    "        return self.classifier(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Networks for Speaker Verification\n",
    "Siamese networks represent an extension that's particularly useful for speaker verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_network(input_shape):\n",
    "    # Base network\n",
    "    base_network = Sequential([\n",
    "        Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(3),\n",
    "        Conv1D(128, 3, activation='relu'),\n",
    "        MaxPooling1D(3),\n",
    "        Conv1D(256, 3, activation='relu'),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(128, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    # Create twin networks\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    \n",
    "    # Process both inputs through same network\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    # Calculate distance between outputs\n",
    "    distance = Lambda(lambda x: K.abs(x[0] - x[1]))([processed_a, processed_b])\n",
    "    \n",
    "    # Output prediction\n",
    "    prediction = Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "    \n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Supervised Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data augmentation here is different than the data augmentation that happened before running the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss function for the self-supervised model\n",
    "    \"\"\"\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def data_augmentation(x):\n",
    "    \"\"\"\n",
    "    Simple data augmentation function to create different views of the same input\n",
    "    \"\"\"\n",
    "    # Add random noise\n",
    "    x_aug = x + tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=0.1)\n",
    "    \n",
    "    # Random time masking\n",
    "    batch_size, time_steps, features = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
    "    mask_length = tf.random.uniform([], 0, tf.cast(time_steps * 0.2, tf.int32), dtype=tf.int32)\n",
    "    mask_start = tf.random.uniform([], 0, time_steps - mask_length, dtype=tf.int32)\n",
    "    \n",
    "    mask = tf.concat([\n",
    "        tf.ones((batch_size, mask_start, features)),\n",
    "        tf.zeros((batch_size, mask_length, features)),\n",
    "        tf.ones((batch_size, time_steps - mask_start - mask_length, features))\n",
    "    ], axis=1)\n",
    "    \n",
    "    x_aug = x_aug * mask\n",
    "    return x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveAudioEncoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder architecture\n",
    "        self.encoder = Sequential([\n",
    "            Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            MaxPooling2D(),\n",
    "            Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            MaxPooling2D(),\n",
    "            Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(128)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.encoder(x)\n",
    "        \n",
    "    def data_augmentation(self, x):\n",
    "        \"\"\"Simple data augmentation for spectrograms\"\"\"\n",
    "        # Add random noise\n",
    "        x_aug = x + tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=0.1)\n",
    "        return x_aug\n",
    "    \n",
    "    def contrastive_loss(self, anchor, positive, temperature=0.1):\n",
    "        \"\"\"Compute NT-Xent loss for contrastive learning\"\"\"\n",
    "        # Normalize the embeddings\n",
    "        anchor = tf.math.l2_normalize(anchor, axis=1)\n",
    "        positive = tf.math.l2_normalize(positive, axis=1)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = tf.matmul(anchor, positive, transpose_b=True) / temperature\n",
    "        \n",
    "        # Labels are just the diagonal elements (positive pairs)\n",
    "        batch_size = tf.shape(anchor)[0]\n",
    "        labels = tf.range(batch_size)\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=similarity))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        # Unpack the data\n",
    "        anchor, positive = data\n",
    "        \n",
    "        # Apply data augmentation to create slightly different views\n",
    "        anchor_aug = self.data_augmentation(anchor)\n",
    "        positive_aug = self.data_augmentation(positive)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get the encodings\n",
    "            anchor_encoding = self.encoder(anchor_aug)\n",
    "            positive_encoding = self.encoder(positive_aug)\n",
    "            \n",
    "            # Calculate contrastive loss\n",
    "            loss = self.contrastive_loss(anchor_encoding, positive_encoding)\n",
    "            \n",
    "        # Get gradients and update weights\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of CRNN model\n",
    "# Assuming your spectrograms have shape (128, 128, 1) and 2 output classes\n",
    "input_shape = (128, 128, 1)  # Mel spectrogram dimensions\n",
    "num_classes = 2  # Binary classification (Jeevan vs Not_Jeevan or English vs Not_English)\n",
    "\n",
    "crnn_model = build_crnn_model(input_shape, num_classes)\n",
    "crnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "crnn_model.summary()\n",
    "\n",
    "# Example of how you would train (uncomment when you have data ready)\n",
    "# history = crnn_model.fit(\n",
    "#    train_dataset,\n",
    "#    validation_data=val_dataset,\n",
    "#    epochs=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of Attention model\n",
    "attention_model = build_attention_audio_model(input_shape, num_classes)\n",
    "attention_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the PyTorch Wav2Vec2 model\n",
    "# This requires PyTorch data preparation\n",
    "import torch\n",
    "\n",
    "# Example input dimensions\n",
    "batch_size = 4\n",
    "input_length = 16000  # 1 second of audio at 16kHz\n",
    "\n",
    "# Sample random input tensor (this would be your actual audio data)\n",
    "sample_input = torch.randn(batch_size, input_length)\n",
    "\n",
    "# Initialize the model\n",
    "classifier = AudioClassifier(num_classes=2)\n",
    "\n",
    "# Forward pass\n",
    "outputs = classifier(sample_input)\n",
    "print(f\"Output shape: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of Siamese Network\n",
    "# For audio features, typically we'd use a 1D representation\n",
    "# like MFCC features with shape (time_steps, num_features)\n",
    "input_shape = (128, 13)  # Example: 128 time steps with 13 MFCC features\n",
    "\n",
    "siamese_model = build_siamese_network(input_shape)\n",
    "siamese_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of Contrastive Learning model\n",
    "contrastive_model = ContrastiveAudioEncoder()\n",
    "contrastive_model.compile(optimizer='adam')\n",
    "\n",
    "# Example of how you would create a pair of samples (original + augmented)\n",
    "# This is a simplified example - you'd need actual spectrogram data\n",
    "batch_size = 4\n",
    "height, width = 128, 128\n",
    "example_spectrograms = tf.random.normal((batch_size, height, width, 1))\n",
    "\n",
    "# Show that the model can process inputs\n",
    "embeddings = contrastive_model(example_spectrograms)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
