{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Audio Classification: Speaker & Language\n","\n","This notebook does the following:\n","1. Use a single **metadata CSV** to label both **speaker** and **language**.\n","2. Extract **MFCC features** from each audio file.\n","3. Perform **binary classification** separately for:\n","   - **Speaker** (Jeevan vs. Not_Jeevan)\n","   - **Language** (English vs. Not_English)\n","4. Use **k-Fold Cross-Validation** to evaluate performance (accuracy, precision, recall, F1) and generate confusion matrices."]},{"cell_type":"code","execution_count":2,"id":"cda41d9e","metadata":{},"outputs":[],"source":["#Importing ALl libraries\n","import os\n","import pandas as pd\n","import librosa\n","from sklearn.model_selection import StratifiedKFold, cross_val_predict\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, confusion_matrix\n","import numpy as np"]},{"cell_type":"markdown","id":"1a4b90b8","metadata":{},"source":["# Data Collection\n","__ Add Text Here\n","\n","* Youtube\n","* Google Photos\n","* Facebook and Messenger\n","* Recorded to balance Data"]},{"cell_type":"markdown","id":"37680efa","metadata":{},"source":["# Data Pre Processing\n","__ Add text here\n","\n","And code if needed. Or link to Github\n","* Used yt_dlp to download just the audio from youtube videos (These videos contain just my speech). No copyright infriengement intended. The channel used is in the codecell.\n","* Used __ to convert all video into audio\n","* Used librosa, ffemg to trim audio data into equal slices of 7 seconds (source for why that's suitable), removed silence, and added empty voice when a segment would be less than 7 seconds.\n","\n","## What needs to be done:\n","These factors might result in some inaccuracy.\n","* The audio is recorded from a phone, often with a lot of wind or people in the background. So, there's some noise that needs to be cleaned.\n","* Some data is labeled as one class but has data from both classes. \n","    * In conversations when both people are speaking in one segment, I classified it based on whoever is speaking for a longer time duration. \n","    * People who speak Nepali also sprinkle English words in between, again I used the majority time rule to label data."]},{"cell_type":"markdown","id":"6ba11ef9","metadata":{},"source":["# Data Labeling\n","The most challenging part after fining the data was labeling.\n","\n","__ Add more__"]},{"cell_type":"markdown","id":"99eff8cc","metadata":{},"source":["# Data Analysis"]},{"cell_type":"code","execution_count":3,"id":"68e79255","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Matplotlib is building the font cache; this may take a moment.\n"]}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","\n","# Load the metadata CSV file into a pandas dataframe\n","# If your CSV file has no header, we assign our own column names.\n","df = pd.read_csv('data/metadata.csv', header=None, names=['filename', 'speaker', 'language'])\n","\n","# Display basic statistics about the dataframe\n","print(\"Total files:\", len(df))\n","print(\"\\nData Sample:\")\n","print(df.head())\n","\n","# Function to extract category and segment number from the filename.\n","def extract_info(filename):\n","    # Matches strings like \"Jeevan_Jaycees_Nepali_segment_1.wav\"\n","    # and extracts:\n","    #   category: \"Jeevan_Jaycees_Nepali_segment\"\n","    #   segment: 1\n","    match = re.search(r'(.*_segment)_(\\d+)\\.wav', filename)\n","    if match:\n","        category = match.group(1)\n","        segment = int(match.group(2))\n","        return pd.Series([category, segment])\n","    else:\n","        return pd.Series([None, None])\n","\n","# Apply the extraction function to create two new columns: 'category' and 'segment'\n","df[['category', 'segment']] = df['filename'].apply(extract_info)\n","\n","# Print unique categories and their counts\n","print(\"\\nUnique Categories:\")\n","print(df['category'].unique())\n","\n","print(\"\\nCategory Counts:\")\n","print(df['category'].value_counts())\n","\n","# Basic stats on segment numbers\n","print(\"\\nSegment Number Statistics:\")\n","print(df['segment'].describe())\n","\n","# Visualization 1: Bar plot of file counts per category\n","plt.figure(figsize=(10, 6))\n","sns.countplot(data=df, x='category', order=df['category'].value_counts().index)\n","plt.title(\"File Count per Category\")\n","plt.xlabel(\"Category\")\n","plt.ylabel(\"Count\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n","\n","# Visualization 2: Histogram of segment numbers, colored by category\n","plt.figure(figsize=(10, 6))\n","sns.histplot(data=df, x='segment', hue='category', multiple='stack', bins=30)\n","plt.title(\"Distribution of Segment Numbers by Category\")\n","plt.xlabel(\"Segment Number\")\n","plt.ylabel(\"Count\")\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false},"outputs":[],"source":["\n","def extract_features(file_path, sr=16000, n_mfcc=13):\n","    \"\"\"\n","    Loads the audio file, extracts MFCC features, and returns the averaged MFCCs.\n","    \"\"\"\n","    y, sr = librosa.load(file_path, sr=sr)\n","    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n","    mfcc_mean = np.mean(mfcc, axis=1)\n","    return mfcc_mean\n","\n","def build_dataset(metadata_csv, audio_dir):\n","    \"\"\"\n","    Reads metadata from a CSV that has at least 3 columns:\n","        - filename\n","        - speaker_label (e.g., \"Jeevan\" or \"Not_Jeevan\")\n","        - language_label (e.g., \"English\" or \"Not_English\")\n","    \"\"\"\n","    df = pd.read_csv(metadata_csv)\n","    X, y_speaker, y_language = [], [], []\n","    for _, row in df.iterrows():\n","        file_path = os.path.join(audio_dir, row['filename'])\n","        X.append(extract_features(file_path))\n","        y_speaker.append(1 if row['speaker_label'].lower() == 'jeevan' else 0)\n","        y_language.append(1 if row['language_label'].lower() == 'english' else 0)\n","    return np.array(X), np.array(y_speaker), np.array(y_language)\n","\n","def evaluate_classifier(X, y, n_splits=5):\n","    \"\"\"\n","    Performs k-Fold cross-validation, returns classification metrics and a confusion matrix.\n","    \"\"\"\n","    clf = SVC(kernel='linear', probability=True, random_state=42)\n","    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    y_pred = cross_val_predict(clf, X, y, cv=skf)\n","    return classification_report(y, y_pred, target_names=[\"Class 0\", \"Class 1\"]), confusion_matrix(y, y_pred)"]},{"cell_type":"code","execution_count":10,"id":"cba31681","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m audio_directory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/audio_files\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Build dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X, y_speaker, y_language \u001b[39m=\u001b[39m build_dataset(metadata_csv_path, audio_directory)\n","\u001b[1;32m/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mbuild_dataset\u001b[39m(metadata_csv, audio_dir):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    Reads metadata from a CSV that has at least 3 columns:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m        - filename\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m        - speaker_label (e.g., \"Jeevan\" or \"Not_Jeevan\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m        - language_label (e.g., \"English\" or \"Not_English\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(metadata_csv)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     X, y_speaker, y_language \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["# Paths to metadata CSV and audio files folder\n","metadata_csv_path = \"data/metadata.csv\"\n","audio_directory = \"data/audio_files\"\n","\n","# Build dataset\n","X, y_speaker, y_language = build_dataset(metadata_csv_path, audio_directory)"]},{"cell_type":"code","execution_count":11,"id":"33084131","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'X' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Speaker Classification\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m speaker_report, speaker_cm \u001b[39m=\u001b[39m evaluate_classifier(X, y_speaker, n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=== Speaker Classification (Jeevan vs. Not_Jeevan) ===\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jeevanbhatta/Minerva/Machine-Learning-CS156/voice-classification/audio_classification.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mClassification Report:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, speaker_report)\n","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"]}],"source":["# Speaker Classification\n","speaker_report, speaker_cm = evaluate_classifier(X, y_speaker, n_splits=5)\n","print(\"=== Speaker Classification (Jeevan vs. Not_Jeevan) ===\")\n","print(\"Classification Report:\\n\", speaker_report)\n","print(\"Confusion Matrix:\\n\", speaker_cm)\n"]},{"cell_type":"code","execution_count":null,"id":"417f1125","metadata":{},"outputs":[],"source":["# Language Classification\n","language_report, language_cm = evaluate_classifier(X, y_language, n_splits=5)\n","print(\"\\n=== Language Classification (English vs. Not_English) ===\")\n","print(\"Classification Report:\\n\", language_report)\n","print(\"Confusion Matrix:\\n\", language_cm)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"name":"AudioClassificationNotebook"},"nbformat":4,"nbformat_minor":5}
